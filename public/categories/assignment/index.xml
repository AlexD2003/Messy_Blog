<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Assignment on My messy collection</title>
    <link>//localhost:1313/categories/assignment/</link>
    <description>Recent content in Assignment on My messy collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn</language>
    <copyright>Copyright © 2008–2019</copyright>
    <lastBuildDate>Fri, 30 Aug 2024 16:03:22 +0300</lastBuildDate>
    
	<atom:link href="//localhost:1313/categories/assignment/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tecknoworks-Intenship challange</title>
      <link>//localhost:1313/post/tecknoworks_challange/</link>
      <pubDate>Fri, 30 Aug 2024 16:03:22 +0300</pubDate>
      
      <guid>//localhost:1313/post/tecknoworks_challange/</guid>
      <description>&lt;h1 id=&#34;premise&#34;&gt;Premise&lt;/h1&gt;
&lt;h4 id=&#34;overview&#34;&gt;Overview&lt;/h4&gt;
&lt;p&gt;This technical project is designed to evaluate your technical skills, creativity, and problem-solving abilities
in both Data Engineering and Artificial Intelligence. The project will help us assess how well you
understand the concepts and tools you&amp;rsquo;ll be working with during the internship. The project should take
no more than 2 to 3 hours to complete.&lt;/p&gt;
&lt;h4 id=&#34;project-description&#34;&gt;Project Description&lt;/h4&gt;
&lt;p&gt;You are tasked with designing a simple data pipeline using Microsoft Fabric and performing data science
tasks using Python. Most likely, you don&amp;rsquo;t have experience with Microsoft Fabric. We&amp;rsquo;re interested in how
your mind works when you have to face a challenge like this. Microsoft Fabric is a comprehensive data
analytics platform that integrates data engineering, data science, data warehousing, and business
intelligence in one environment. By using Microsoft Fabric, we expect our interns to gain hands-on
experience with a tool that Microsoft expects to be widely adopted in the industry. This exposure not only
makes you more competitive in the job market but also prepares you for real-world scenarios where you
are likely to encounter similar integrated platforms.&lt;br&gt;
Microsoft Fabric is a paid tool, so in order to solve these exercises, you have to simply read the
documentation to have an introduction to the tool, watch a few tutorials if needed and answer the
questions based on your findings and your prior knowledge from University courses. You don&amp;rsquo;t need to
access Fabric for this basic assignment. You don&amp;rsquo;t have to implement the solution, only to design it based
on the documentation that you read. You can use text descriptions or any type of visual representation to
answer the questions (e.g. Dataflow diagrams) for Section 1. For Section 2 you can implement the solution
for free using Google Colab or any other local python environment.&lt;br&gt;
The project is divided into two main sections: Data Engineering and Artificial Intelligence.&lt;/p&gt;
&lt;h1 id=&#34;section-1&#34;&gt;Section 1&lt;/h1&gt;
&lt;h4 id=&#34;scenario&#34;&gt;Scenario&lt;/h4&gt;
&lt;p&gt;You are a new data engineer at a retail company. The company has a dataset containing information on
customer transactions. Your task is to design and implement a data pipeline that ingests, processes, and
stores this data using Microsoft Fabric.&lt;br&gt;
The dataset is available here: &lt;a href=&#34;https://www.kaggle.com/datasets/fahadrehman07/retail-transaction-dataset?resource=download&#34;&gt;https://www.kaggle.com/datasets/fahadrehman07/retail-transaction-dataset?resource=download&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;task-1-premise&#34;&gt;Task 1 premise&lt;/h4&gt;
&lt;p&gt;Question: First, you have to bring this data into Fabric. How do you envision this? What will be the expected
steps? How exactly will it be stored? Is the data in a suitable format and size to ingest in Fabric? What can
you do related to storage in order to improve the performance of the future data queries if the size of the
dataset is expected to grow in time (hint: e.g. partition the data)?
(Answer with text descriptions or diagrams)&lt;/p&gt;
&lt;h4 id=&#34;task-1-solution&#34;&gt;Task 1 solution&lt;/h4&gt;
&lt;p&gt;Firstly, I will begin by looking at the data set given in the premise in order to get a feeling for what I am working with.&lt;br&gt;
The data set is presented in &lt;strong&gt;csv format and has a size of around 13MB (important for later)&lt;/strong&gt;. It seems to be a &lt;strong&gt;retail transaction data set&lt;/strong&gt;, basically oferring you a peak at common consumer purchase patterns with the present collumns being : &lt;strong&gt;CustomerId, ProductId, Quantity, Price, TransactionDate, PaymentMethod, StoreLocation, ProductCategory, AppliedDiscount and TotalAmount&lt;/strong&gt;.&lt;br&gt;
Since the data set is given in csv format which is compatible with the import feature of &lt;strong&gt;Microsoft Fabric&lt;/strong&gt; and the size is not significant, the import should be pretty straight forward. We should plan for further expansion tho.&lt;br&gt;
&lt;strong&gt;Okay, how how do I import it?&lt;/strong&gt; Firstly, in what? A quick look around google later, the most obvious choice was between a &lt;strong&gt;Data Lake or a Data Warehouse&lt;/strong&gt;. I would go with a &lt;strong&gt;Data Warehouse&lt;/strong&gt; since it is meant for already structured data (exactly like my data set) while a &lt;strong&gt;Data Lake&lt;/strong&gt; is more inclined towards raw data meant for analysis and further implications in various projects. &lt;a href=&#34;https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-a-data-lake#:~:text=risks%20more%20efficiently.-,What&#39;s%20the%20difference%20between%20a%20data%20lake%20and%20a%20data,as%20specific%20BI%20use%20cases.&#34;&gt;Source.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>