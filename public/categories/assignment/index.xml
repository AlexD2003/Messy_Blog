<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Assignment on My messy collection</title>
    <link>//localhost:1313/categories/assignment/</link>
    <description>Recent content in Assignment on My messy collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>cn</language>
    <copyright>Copyright © 2008–2019</copyright>
    <lastBuildDate>Fri, 30 Aug 2024 16:03:22 +0300</lastBuildDate>
    
	<atom:link href="//localhost:1313/categories/assignment/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tecknoworks-Intenship challange</title>
      <link>//localhost:1313/post/tecknoworks_challange/</link>
      <pubDate>Fri, 30 Aug 2024 16:03:22 +0300</pubDate>
      
      <guid>//localhost:1313/post/tecknoworks_challange/</guid>
      <description>&lt;h1 id=&#34;premise&#34;&gt;Premise&lt;/h1&gt;
&lt;h4 id=&#34;overview&#34;&gt;Overview&lt;/h4&gt;
&lt;p&gt;This technical project is designed to evaluate your technical skills, creativity, and problem-solving abilities
in both Data Engineering and Artificial Intelligence. The project will help us assess how well you
understand the concepts and tools you&amp;rsquo;ll be working with during the internship. The project should take
no more than 2 to 3 hours to complete.&lt;/p&gt;
&lt;h4 id=&#34;project-description&#34;&gt;Project Description&lt;/h4&gt;
&lt;p&gt;You are tasked with designing a simple data pipeline using Microsoft Fabric and performing data science
tasks using Python. Most likely, you don&amp;rsquo;t have experience with Microsoft Fabric. We&amp;rsquo;re interested in how
your mind works when you have to face a challenge like this. Microsoft Fabric is a comprehensive data
analytics platform that integrates data engineering, data science, data warehousing, and business
intelligence in one environment. By using Microsoft Fabric, we expect our interns to gain hands-on
experience with a tool that Microsoft expects to be widely adopted in the industry. This exposure not only
makes you more competitive in the job market but also prepares you for real-world scenarios where you
are likely to encounter similar integrated platforms.&lt;br&gt;
Microsoft Fabric is a paid tool, so in order to solve these exercises, you have to simply read the
documentation to have an introduction to the tool, watch a few tutorials if needed and answer the
questions based on your findings and your prior knowledge from University courses. You don&amp;rsquo;t need to
access Fabric for this basic assignment. You don&amp;rsquo;t have to implement the solution, only to design it based
on the documentation that you read. You can use text descriptions or any type of visual representation to
answer the questions (e.g. Dataflow diagrams) for Section 1. For Section 2 you can implement the solution
for free using Google Colab or any other local python environment.&lt;br&gt;
The project is divided into two main sections: Data Engineering and Artificial Intelligence.&lt;/p&gt;
&lt;h1 id=&#34;section-1&#34;&gt;Section 1&lt;/h1&gt;
&lt;h4 id=&#34;scenario&#34;&gt;Scenario&lt;/h4&gt;
&lt;p&gt;You are a new data engineer at a retail company. The company has a dataset containing information on
customer transactions. Your task is to design and implement a data pipeline that ingests, processes, and
stores this data using Microsoft Fabric.&lt;br&gt;
The dataset is available here: &lt;a href=&#34;https://www.kaggle.com/datasets/fahadrehman07/retail-transaction-dataset?resource=download&#34;&gt;https://www.kaggle.com/datasets/fahadrehman07/retail-transaction-dataset?resource=download&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;task-1-premise&#34;&gt;Task 1 premise&lt;/h4&gt;
&lt;p&gt;Question: First, you have to bring this data into Fabric. How do you envision this? What will be the expected
steps? How exactly will it be stored? Is the data in a suitable format and size to ingest in Fabric? What can
you do related to storage in order to improve the performance of the future data queries if the size of the
dataset is expected to grow in time (hint: e.g. partition the data)?
(Answer with text descriptions or diagrams)&lt;/p&gt;
&lt;h4 id=&#34;task-1-solution&#34;&gt;Task 1 solution&lt;/h4&gt;
&lt;p&gt;Firstly, I will begin by looking at the data set given in the premise in order to get a feeling for what I am working with.&lt;br&gt;
The data set is presented in &lt;strong&gt;csv format and has a size of around 13MB (important for later)&lt;/strong&gt;. It seems to be a &lt;strong&gt;retail transaction data set&lt;/strong&gt;, basically offering you a peek at common consumer purchase patterns with the present columns being : &lt;strong&gt;CustomerId, ProductId, Quantity, Price, TransactionDate, PaymentMethod, StoreLocation, ProductCategory, AppliedDiscount and TotalAmount&lt;/strong&gt;.&lt;br&gt;
Since the data set is given in csv format which is compatible with the import feature of &lt;strong&gt;Microsoft Fabric&lt;/strong&gt; and the size is not significant, the import should be pretty straight forward. We should plan for further expansion tho.&lt;br&gt;
&lt;strong&gt;Okay, how how do I import it?&lt;/strong&gt; Firstly, in what? A quick look around google later, the most obvious choice was between a &lt;strong&gt;Data Lake or a Data Warehouse&lt;/strong&gt;. I would go with a &lt;strong&gt;Data Warehouse&lt;/strong&gt; since it is meant for already structured data (exactly like my data set) while a &lt;strong&gt;Data Lake&lt;/strong&gt; is more inclined towards raw data meant for analysis and further implications in various projects. &lt;a href=&#34;https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-a-data-lake#:~:text=risks%20more%20efficiently.-,What&#39;s%20the%20difference%20between%20a%20data%20lake%20and%20a%20data,as%20specific%20BI%20use%20cases.&#34;&gt;Source.&lt;/a&gt;&lt;br&gt;
In order to import this data into Microsoft Fabric we could setup a pipeline. &lt;strong&gt;Pipeline setup:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Configure an automated data pipeline within Microsoft Fabric.&lt;/li&gt;
&lt;li&gt;Source : We should point the pipeline towards the source location of the &lt;strong&gt;csv file&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Transformation : We should always think about transforming data when we are doing an import in order to further suit our needs, for example remove unnecessary columns or filter just what we need.&lt;/li&gt;
&lt;li&gt;Destination : Finally, we set the pipeline to store the ingested data into our Data Warehouse, as explained earlier.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;source&#34;&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=LoESCJpru8w&#34;&gt;Source.&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;What can you do related to storage in order to improve the performance of the future data queries if the size of the dataset is expected to grow in time (hint: e.g. partition the data)?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the ingested data is stored in a &lt;strong&gt;Data Warehouse&lt;/strong&gt;, it is prone to efficient querying. In order to further improve the performance, from my work experience, I would suggest partitioning the data by &lt;strong&gt;TransactionDate&lt;/strong&gt; and possibly by &lt;strong&gt;ProductId&lt;/strong&gt;. Additionally, converting the data into Parquet format will improve storage efficiency. &lt;a href=&#34;https://www.macrometa.com/articles/what-is-data-partitioning&#34;&gt;Source.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;data-flow-diagram-&#34;&gt;Data flow diagram :&lt;/h4&gt;
&lt;img src=&#34;//localhost:1313/img/dataingdiagram.png&#34; alt=&#34;gif&#34; style=&#34;display: block; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;br&gt;
&lt;h4 id=&#34;task-2-premise&#34;&gt;Task 2 premise&lt;/h4&gt;
&lt;p&gt;Question: Like most of the datasets you&amp;rsquo;ll find in your practice, this dataset has some data issues that
should be corrected. You need to find a way within Microsoft Fabric to perform the following
transformations:&lt;br&gt;
Separate TransactionDate column in 2 distinct columns for Date and Time.&lt;br&gt;
Aggregate the TotalAmount spent by each customer per month.&lt;br&gt;
Replace the &amp;ldquo;Home Decor&amp;rdquo; values from the ProductCategory column with &amp;ldquo;Home Products&amp;rdquo;.&lt;br&gt;
Create a new column HighValueCustomer that is a boolean column that assigns True or False based on
your own rule. Think about a rule with a logic that makes sense in the context.&lt;br&gt;
Load the transformed data into a new table in your Data Lake.&lt;br&gt;
How do you envision solving this based on your research on Microsoft Fabric? No implementation is
needed.&lt;br&gt;
(Answer with text descriptions, images or diagrams).&lt;/p&gt;
&lt;h4 id=&#34;task-2-solution&#34;&gt;Task 2 solution&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Separate TransactionDate into 2 columns.&lt;/strong&gt; As I explained previously, before sending the ingested data to the Data Warehouse we should parse it through a Data Transformation step. In order to achieve what is asked, there are Microsoft data pipeline capabilities that could get the expected result. Data pipelines in Fabric are designed to allow for complex data manipulations and transformations as part of the data flow. Within the pipeline, we could add a transformation step for the received data. Fabric allows for transformations through both low-code/no-code interfaces and SQL-based transformations and for this task, I think that a simple SQL snippet should suffice. &lt;a href=&#34;https://learn.microsoft.com/en-us/fabric/data-factory/transform-data&#34;&gt;Source.&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;sql-snippet-&#34;&gt;SQL snippet :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SELECT 
    CustomerID,
    ProductID,
    Quantity,
    Price,
    CAST(TransactionDate AS DATE) AS Date,
    CAST(TransactionDate AS TIME) AS Time,
    etc.
FROM 
    RetailTransactions
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Explanation :&lt;/strong&gt; Casting the datetime variable as DATE and TIME respectively, we will only get the DATE and TIME part of the initial value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aggregate the TotalAmount spent by each customer per month.&lt;/strong&gt; The first step would be separating the &lt;strong&gt;Year&lt;/strong&gt; and the &lt;strong&gt;Month&lt;/strong&gt; values from &lt;strong&gt;TransactionDate&lt;/strong&gt; in order to differentiate between the entries. We should use a transformation step in the pipepline to create new entries just like we did last time.&lt;/p&gt;
&lt;h4 id=&#34;sql-snippet--1&#34;&gt;SQL snippet :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SELECT 
    CustomerId,
    YEAR(TransactionDate) AS Year,
    MONTH(TransactionDate) AS Month,
    TotalAmount
FROM 
    RetailTransactions
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that we have extracted the relevant information, we could agregate the TotalAmount for each customer on a monthly basis.&lt;br&gt;
Step 1 : Add an aggregation component to the pipeline.&lt;br&gt;
Step 2 : Configure the aggregation component.&lt;/p&gt;
&lt;h4 id=&#34;sql-snippet--2&#34;&gt;SQL snippet :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SELECT 
    CustomerId,
    YEAR(TransactionDate) AS Year,
    MONTH(TransactionDate) AS Month,
    SUM(TotalAmount) AS TotalSpent
FROM 
    RetailTransactions
GROUP BY 
    CustomerId, 
    YEAR(TransactionDate),
    MONTH(TransactionDate);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This basically just sums the TotalAmount based on each month.&lt;br&gt;
As always, after getting the result we should do some data validation in order to make sure that we have consistency.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Replace the &amp;ldquo;Home Decor&amp;rdquo; values from the ProductCategory column with &amp;ldquo;Home Products&amp;rdquo;.&lt;/strong&gt; This will be pretty straight forward since we could just use a case when statement. We should insert a transformation component in the pipeline with the directive of changing the values in ProductCategory just when the case is met.&lt;/p&gt;
&lt;h4 id=&#34;sql-snippet--3&#34;&gt;SQL snippet :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CASE 
    WHEN ProductCategory = &amp;#39;Home Decor&amp;#39; THEN &amp;#39;Home Products&amp;#39;
    ELSE ProductCategory
END AS ProductCategory
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Create a new column HighValueCustomer that is a boolean column that assigns True or False based on your own rule. Think about a rule with a logic that makes sense in the context.&lt;/strong&gt; The goal for this task is to create a HighValueCustomer column in my database. A simple rule I would personally think of would be that a customer who has spent more than 10k should be moved to the &lt;strong&gt;High Value Customer&lt;/strong&gt; status. Using a data pipeline we could add a data transformation step in order to filter our customers based on our new created rule.&lt;/p&gt;
&lt;h4 id=&#34;sql-snippet--4&#34;&gt;SQL snippet :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SELECT 
    CustomerId,
    SUM(TotalAmount) AS TotalSpent
FROM 
    RetailTransactions
GROUP BY 
    CustomerId;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And in order to actually sort the data and filter for the sum greater than 10k we could do something like this :&lt;/p&gt;
&lt;h4 id=&#34;sql-snippet--5&#34;&gt;SQL snippet :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CASE 
    WHEN TotalSpent &amp;gt; 10000 THEN &amp;#39;True&amp;#39;
    ELSE &amp;#39;False&amp;#39;
END AS HighValueCustomer
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Load the transformed data into a new table in your Data Lake.&lt;/strong&gt; As I explained earlier, I would use a Data Warehouse but the process of transforming the data into a new table should be identical. Firstly, defining the table is crucial. The table should have the same fields i.e. CustomerId, Year and so on and if we are using a pipeline, the pipeline can include a step that automatically creates the table based on the schema defined by my transformed data.&lt;/p&gt;
&lt;h4 id=&#34;task-3-solution&#34;&gt;Task 3 solution&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question: How will you use Microsoft Fabric to create a simple dashboard?&lt;/strong&gt;
After doing a little bit of research, the most obvious first step in creating this sort of dashboard should be sanitizing and validating the data. After you are sure that the data you are working with is indeed accurate we should exploit the fact that Microsoft Fabric integrates with Power BI. We could navigate to the Power BI section in Microsoft Fabric and simply choose &lt;strong&gt;Create&lt;/strong&gt; and then &lt;strong&gt;Report&lt;/strong&gt;. The next obvious step would be connecting this Report to the prepared dataset stored in our Data Warehouse. The final step is just using the &lt;strong&gt;Get Data&lt;/strong&gt; feature in Power BI in order to ingest the stored data. Now all that is left to do is design an intuitive layout in order to display your findings. I also created a mock dashboard implementation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A chart for total monthly sales by product category &amp;amp; a chart showing the number of
HighValueCustomers over time.&lt;/strong&gt;&lt;/p&gt;
&lt;img src=&#34;//localhost:1313/img/dashboard.png&#34; alt=&#34;gif&#34; style=&#34;display: block; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;br&gt;
&lt;h1 id=&#34;section-2&#34;&gt;Section 2&lt;/h1&gt;
&lt;h4 id=&#34;task-1&#34;&gt;Task 1&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Load the data you created into a Pandas DataFrame. Create new features such as the total amount spent
in the last 3 months, the average transaction amount, and the number of distinct product categories
purchased, etc.
Create a target variable IsHighValueNextMonth which is True if the customer becomes a
HighValueCustomer in the next month, otherwise False. Clean the dataset if needed.&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;solution-&#34;&gt;Solution :&lt;/h4&gt;
&lt;p&gt;Let`s start by loading the csv data into a dataframe in order to see if it picks it up.&lt;/p&gt;
&lt;h4 id=&#34;code-&#34;&gt;Code :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import pandas as pd

# Load phase
df = pd.read_csv(&amp;#39;Retail_Transaction_Dataset.csv&amp;#39;)

# Convert TransactionDate to datetime
df[&amp;#39;TransactionDate&amp;#39;] = pd.to_datetime(df[&amp;#39;TransactionDate&amp;#39;])

# Simple sort
df.sort_values(by=[&amp;#39;CustomerID&amp;#39;, &amp;#39;TransactionDate&amp;#39;], inplace=True)

# Print check
print(df.head())
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;code-run-&#34;&gt;Code run :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;06:03:38 archie@Archie randomPy → python3 ai.py
       CustomerID ProductID  Quantity      Price     TransactionDate PaymentMethod                             StoreLocation ProductCategory  DiscountApplied(%)  TotalAmount
81519          14         A         5  60.649721 2023-08-06 06:45:00   Credit Card          Unit 3258 Box 4585\nDPO AE 23889           Books           15.504050   256.232791
80034          42         B         7  88.861581 2023-05-19 21:52:00        PayPal  565 Ashlee Lock\nStephaniefort, CA 33648      Home Decor           19.191090   502.656523
24358          49         A         1  25.152676 2023-06-05 13:10:00    Debit Card               USCGC Anthony\nFPO AP 93458     Electronics           14.923377    21.399047
57189          59         B         4  37.424038 2023-08-19 03:50:00    Debit Card             USCGC Carpenter\nFPO AE 70852        Clothing            6.736390   139.612036
86585          59         D         8  14.867200 2024-04-01 01:06:00    Debit Card     1854 Bailey Dam\nWest April, NC 97920      Home Decor            7.614866   109.880660
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The next step should be implementing the new features. Since they are simple interogations and sorts it shouldn`t be very difficult.&lt;/p&gt;
&lt;h4 id=&#34;total-spent-in-the-last-3-months-&#34;&gt;Total spent in the last 3 months :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df[&amp;#39;TotalSpentLast3Months&amp;#39;] = df.groupby(&amp;#39;CustomerID&amp;#39;).apply(
    lambda x: x.set_index(&amp;#39;TransactionDate&amp;#39;).rolling(&amp;#39;90D&amp;#39;)[&amp;#39;TotalAmount&amp;#39;].sum()
).reset_index(level=0, drop=True)
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;average-transaction-amount-&#34;&gt;Average transaction amount :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df[&amp;#39;AvgTransactionAmount&amp;#39;] = df.groupby(&amp;#39;CustomerID&amp;#39;)[&amp;#39;TotalAmount&amp;#39;].expanding().mean().reset_index(level=0, drop=True)
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;number-of-distinct-categories-&#34;&gt;Number of distinct categories :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;distinct_categories = []
categories_set = set()

for category in df[&amp;#39;ProductCategory&amp;#39;]:
    categories_set.add(category)
    distinct_categories.append(len(categories_set))

df[&amp;#39;DistinctCategories&amp;#39;] = distinct_categories
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now, we should create a target variable &lt;strong&gt;IsHighValueNextMonth&lt;/strong&gt; which is true if the customer will become high value in the next month.&lt;br&gt;
Firstly, let`s create the &lt;strong&gt;IsHighValueNextMonth&lt;/strong&gt; column based on the threshold :&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;high_value_threshold = 10000
monthly_spending[&amp;#39;IsHighValueCustomer&amp;#39;] = monthly_spending[&amp;#39;TotalAmount&amp;#39;] &amp;gt; high_value_threshold
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We need to shift the column in order to create the target variable :&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;monthly_spending[&amp;#39;IsHighValueNextMonth&amp;#39;] = monthly_spending.groupby(&amp;#39;CustomerID&amp;#39;)[&amp;#39;IsHighValueCustomer&amp;#39;].shift(-1)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now all that is left to do is merge back into the original dataframe, sanitize the data and check our code for errors.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df = pd.merge(df, monthly_spending[[&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;, &amp;#39;IsHighValueNextMonth&amp;#39;]], 
              how=&amp;#39;left&amp;#39;, on=[&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;])
df[&amp;#39;IsHighValueNextMonth&amp;#39;] = df[&amp;#39;IsHighValueNextMonth&amp;#39;].fillna(False)
df.dropna(inplace=True)
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;code-run--1&#34;&gt;Code run :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;   CustomerID     TransactionDate  TotalAmount  IsHighValueNextMonth
0          14 2023-08-06 06:45:00   256.232791                 False
1          42 2023-05-19 21:52:00   502.656523                 False
2          49 2023-06-05 13:10:00    21.399047                 False
3          59 2023-08-19 03:50:00   139.612036                 False
4          59 2024-04-01 01:06:00   109.880660                 False
5          65 2023-06-18 10:29:00   548.006625                 False
6          87 2023-08-21 15:23:00    41.515905                 False
7          96 2024-04-06 12:35:00   194.356816                 False
8          98 2023-12-20 11:49:00   166.934647                 False
9         100 2023-05-13 17:17:00   710.062576                 False
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;task-2&#34;&gt;Task 2&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Split the data into training and testing sets.
Train a simple model of your choice (e.g., Logistic Regression or Decision Tree) to predict
IsHighValueNextMonth.
Evaluate the model using appropriate metrics such as accuracy, precision, and recall.&lt;/strong&gt;&lt;br&gt;
Let`s break down the implementation step by step. Firstly I loaded the data and only used a small batch since otherwise the process would be killed for intense operations.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df = pd.read_csv(&amp;#39;Retail_Transaction_Dataset.csv&amp;#39;)
df = df.sample(frac=0.1, random_state=42)  # Use 10%
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, I applied a datetime conversion in order to facilitate date-based operations.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;df[&amp;#39;TransactionDate&amp;#39;] = pd.to_datetime(df[&amp;#39;TransactionDate&amp;#39;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After the previous implementation of high value sort, we should prepare the data for modeling.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;X = df.drop(columns=[&amp;#39;IsHighValueNextMonth&amp;#39;, &amp;#39;CustomerID&amp;#39;, &amp;#39;TransactionDate&amp;#39;, &amp;#39;YearMonth&amp;#39;])
y = df[&amp;#39;IsHighValueNextMonth&amp;#39;]
encoder = OneHotEncoder(sparse_output=True, drop=&amp;#39;first&amp;#39;)
X_sparse = encoder.fit_transform(X)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The dataset is split into training and testing sets, with 70% of the data used for training and 30% for testing.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.3, random_state=42)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A logistic regression model is initialized and trained using the training data.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Predictions are made on the testing set (X_test) using the trained logistic regression model.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;y_pred = model.predict(X_test)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, the model is evaluated using accuracy, precision, and recall metrics, which are standard for classification tasks.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f&amp;#39;Accuracy: {accuracy:.2f}&amp;#39;)
print(f&amp;#39;Precision: {precision:.2f}&amp;#39;)
print(f&amp;#39;Recall: {recall:.2f}&amp;#39;)

conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(&amp;#34;\nConfusion Matrix:&amp;#34;)
print(conf_matrix)
print(&amp;#34;\nClassification Report:&amp;#34;)
print(class_report)
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;task-3&#34;&gt;Task 3&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Propose at least one creative feature that could improve the model&amp;rsquo;s performance. Implement this
feature and re-evaluate the model.&lt;/strong&gt;&lt;br&gt;
To improve the model&amp;rsquo;s performance, we can introduce a new feature that provides additional information about the customers&amp;rsquo; behavior. One such creative feature could be the &lt;strong&gt;Change in Spending Behavior&lt;/strong&gt; over the past few months. This feature could capture whether a customer&amp;rsquo;s spending has increased, decreased, or remained stable, which might be a strong indicator of whether they will become a high-value customer in the next month.&lt;br&gt;
In order to implement this, we should just calculate the change in spending behavior and then merge the data back into the original dataframe.&lt;/p&gt;
&lt;h4 id=&#34;code--1&#34;&gt;Code :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# Calculation
monthly_spending[&amp;#39;PrevMonthSpending&amp;#39;] = monthly_spending.groupby(&amp;#39;CustomerID&amp;#39;)[&amp;#39;TotalAmount&amp;#39;].shift(1)
monthly_spending[&amp;#39;SpendingChange&amp;#39;] = (monthly_spending[&amp;#39;TotalAmount&amp;#39;] - monthly_spending[&amp;#39;PrevMonthSpending&amp;#39;]) / monthly_spending[&amp;#39;PrevMonthSpending&amp;#39;]
monthly_spending[&amp;#39;SpendingChange&amp;#39;].fillna(0, inplace=True)  # Replace NaN values with 0 (e.g., for the first month)

# Merge
df = pd.merge(df, monthly_spending[[&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;, &amp;#39;IsHighValueNextMonth&amp;#39;, &amp;#39;SpendingChange&amp;#39;]], 
              how=&amp;#39;left&amp;#39;, on=[&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Suggest a potential business use case for the predictive model in a retail environment.&lt;/strong&gt;&lt;br&gt;
The predictive model can be used to identify customers who are likely to become high-value in the next month, allowing retailers to target these customers with personalized promotions and product recommendations. This strategy can boost customer engagement, increase sales, and improve overall customer retention.&lt;/p&gt;
&lt;h4 id=&#34;full-code-for-this-section-&#34;&gt;Full code for this section :&lt;/h4&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report
from sklearn.preprocessing import OneHotEncoder
from scipy.sparse import csr_matrix

df = pd.read_csv(&amp;#39;Retail_Transaction_Dataset.csv&amp;#39;)
df = df.sample(frac=0.1, random_state=42)
df[&amp;#39;TransactionDate&amp;#39;] = pd.to_datetime(df[&amp;#39;TransactionDate&amp;#39;])
df.sort_values(by=[&amp;#39;CustomerID&amp;#39;, &amp;#39;TransactionDate&amp;#39;], inplace=True)
df[&amp;#39;YearMonth&amp;#39;] = df[&amp;#39;TransactionDate&amp;#39;].dt.to_period(&amp;#39;M&amp;#39;)

monthly_spending = df.groupby([&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;])[&amp;#39;TotalAmount&amp;#39;].sum().reset_index()
high_value_threshold = 100
monthly_spending[&amp;#39;IsHighValueCustomer&amp;#39;] = monthly_spending[&amp;#39;TotalAmount&amp;#39;] &amp;gt; high_value_threshold
monthly_spending[&amp;#39;IsHighValueNextMonth&amp;#39;] = monthly_spending.groupby(&amp;#39;CustomerID&amp;#39;)[&amp;#39;IsHighValueCustomer&amp;#39;].shift(-1)
monthly_spending[&amp;#39;IsHighValueNextMonth&amp;#39;] = monthly_spending[&amp;#39;IsHighValueNextMonth&amp;#39;].fillna(False)

monthly_spending[&amp;#39;PrevMonthSpending&amp;#39;] = monthly_spending.groupby(&amp;#39;CustomerID&amp;#39;)[&amp;#39;TotalAmount&amp;#39;].shift(1)
monthly_spending[&amp;#39;SpendingChange&amp;#39;] = (monthly_spending[&amp;#39;TotalAmount&amp;#39;] - monthly_spending[&amp;#39;PrevMonthSpending&amp;#39;]) / monthly_spending[&amp;#39;PrevMonthSpending&amp;#39;]
monthly_spending[&amp;#39;SpendingChange&amp;#39;].fillna(0, inplace=True)

df = pd.merge(df, monthly_spending[[&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;, &amp;#39;IsHighValueNextMonth&amp;#39;, &amp;#39;SpendingChange&amp;#39;]], 
              how=&amp;#39;left&amp;#39;, on=[&amp;#39;CustomerID&amp;#39;, &amp;#39;YearMonth&amp;#39;])
df.dropna(inplace=True)

X = df.drop(columns=[&amp;#39;IsHighValueNextMonth&amp;#39;, &amp;#39;CustomerID&amp;#39;, &amp;#39;TransactionDate&amp;#39;, &amp;#39;YearMonth&amp;#39;])
y = df[&amp;#39;IsHighValueNextMonth&amp;#39;]

encoder = OneHotEncoder(sparse_output=True, drop=&amp;#39;first&amp;#39;)
X_sparse = encoder.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.3, random_state=42)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f&amp;#39;Accuracy: {accuracy:.2f}&amp;#39;)
print(f&amp;#39;Precision: {precision:.2f}&amp;#39;)
print(f&amp;#39;Recall: {recall:.2f}&amp;#39;)

conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(&amp;#34;\nConfusion Matrix:&amp;#34;)
print(conf_matrix)
print(&amp;#34;\nClassification Report:&amp;#34;)
print(class_report)
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>